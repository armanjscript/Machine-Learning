{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DCGAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tz-hrOxNSZJG"
      },
      "source": [
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import  (Activation, BatchNormalization, Dense, Dropout, Flatten, Reshape)\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import Conv2D, Conv2DTranspose\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "img_rows=28\n",
        "img_cols=28\n",
        "channels=1\n",
        "\n",
        "img_shape=(img_rows, img_cols, channels)\n",
        "\n",
        "z_dim=100"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9U_avlnVoYf"
      },
      "source": [
        "**Build Generator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHtBpwpDVzRZ"
      },
      "source": [
        "def build_generator(z_dim):\n",
        "  model=Sequential()\n",
        "  model.add(Dense(256*7*7,input_dim=z_dim))\n",
        "  model.add(Reshape((7,7,256) , input_dim=z_dim))\n",
        "  model.add(Conv2DTranspose(128,kernel_size=3,strides=2,padding='same'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(LeakyReLU(alpha=0.01))\n",
        "  model.add(Conv2DTranspose(1,kernel_size=3,strides=2,padding='same'))\n",
        "  model.add(Activation('tanh'))\n",
        "\n",
        "  return model"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJ4TH-eRXujT"
      },
      "source": [
        "**Build Discriminator**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwTKGVwjX0Xn"
      },
      "source": [
        "def build_discriminator(img_shape):\n",
        "  model=Sequential()\n",
        "  model.add(Conv2D(32,kernel_size=3,strides=2,input_shape=img_shape,padding='same'))\n",
        "  model.add(LeakyReLU(alpha=0.01))\n",
        "  model.add(Conv2D(64,kernel_size=3,strides=2,input_shape=img_shape,padding='same'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(LeakyReLU(alpha=0.01))\n",
        "  model.add(Conv2D(128,kernel_size=3,strides=2,input_shape=img_shape,padding='same'))\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(LeakyReLU(alpha=0.01))\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "  return model"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBGVaMQPbEzP"
      },
      "source": [
        "**Build GAN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIRsyybwbJQx"
      },
      "source": [
        "def build_gan(generator,discriminator):\n",
        "  model=Sequential()\n",
        "  model.add(generator)\n",
        "  model.add(discriminator)\n",
        "  return model\n",
        "\n",
        "discriminator=build_discriminator(img_shape)\n",
        "discriminator.compile(loss='binary_crossentropy',optimizer=Adam(),metrics=['accuracy'])\n",
        "\n",
        "generator=build_generator(z_dim)\n",
        "discriminator.trainable=False\n",
        "\n",
        "gan=build_gan(generator,discriminator)\n",
        "gan.compile(loss='binary_crossentropy',optimizer=Adam())\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_p3ai58dBOA"
      },
      "source": [
        "**GAN Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kgdd_YSvdFVd"
      },
      "source": [
        "losses=[]\n",
        "accuracies=[]\n",
        "iteration_checkpoints=[]\n",
        "\n",
        "def train(iterations,batch_size,sample_interval):\n",
        "  (X_train,_),(_,_) = mnist.load_data()\n",
        "  X_train = X_train / 127.50 - 1.0\n",
        "  X_train=np.expand_dims(X_train,axis=3)\n",
        "\n",
        "  real = np.ones((batch_size,1))\n",
        "  fake = np.zeros((batch_size,1))\n",
        "\n",
        "  for iteration in range(iterations):\n",
        "    idx=np.random.randint(0,X_train.shape[0],batch_size)\n",
        "    imgs=X_train[idx]\n",
        "\n",
        "    z=np.random.normal(0,1,(batch_size,100))\n",
        "    gen_imgs=generator.predict(z)\n",
        "\n",
        "    d_loss_real=discriminator.train_on_batch(imgs,real)\n",
        "    d_loss_fake=discriminator.train_on_batch(gen_imgs,fake)\n",
        "    d_loss,accuracy=0.5*np.add(d_loss_real,d_loss_fake)\n",
        "\n",
        "    z=np.random.normal(0,1,(batch_size,100))\n",
        "    gen_imgs=generator.predict(z)\n",
        "\n",
        "    g_loss=gan.train_on_batch(z,real)\n",
        "\n",
        "    if (iteration+1) % sample_interval == 0:\n",
        "      losses.append((d_loss, g_loss))                            \n",
        "      accuracies.append(100.0 * accuracy)\n",
        "      iteration_checkpoints.append(iteration + 1)\n",
        "      print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (iteration + 1, d_loss, 100.0 * accuracy, g_loss))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LE5BR4hthkdT"
      },
      "source": [
        "**Run Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7leGDrAhnd6",
        "outputId": "70a8a895-ca63-4a3b-b9f7-96373c913443",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "iterations=2000\n",
        "batch_size=128\n",
        "sample_interval=50\n",
        "\n",
        "train(iterations, batch_size, sample_interval)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.030996]\n",
            "100 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.035417]\n",
            "150 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.036105]\n",
            "200 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.029588]\n",
            "250 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.026443]\n",
            "300 [D loss: 0.000087, acc.: 100.00%] [G loss: 0.027757]\n",
            "350 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.022515]\n",
            "400 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.020867]\n",
            "450 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.024715]\n",
            "500 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.016334]\n",
            "550 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.017785]\n",
            "600 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.018248]\n",
            "650 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.019698]\n",
            "700 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.024148]\n",
            "750 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.020614]\n",
            "800 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.013825]\n",
            "850 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.013743]\n",
            "900 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.017368]\n",
            "950 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.017245]\n",
            "1000 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.021040]\n",
            "1050 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.020567]\n",
            "1100 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.029751]\n",
            "1150 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.077114]\n",
            "1200 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.067243]\n",
            "1250 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.052992]\n",
            "1300 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.053517]\n",
            "1350 [D loss: 0.000004, acc.: 100.00%] [G loss: 0.048334]\n",
            "1400 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.080271]\n",
            "1450 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.124828]\n",
            "1500 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.097115]\n",
            "1550 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.149091]\n",
            "1600 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.108370]\n",
            "1650 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.137538]\n",
            "1700 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.248711]\n",
            "1750 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.094137]\n",
            "1800 [D loss: 0.000003, acc.: 100.00%] [G loss: 0.063196]\n",
            "1850 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.042025]\n",
            "1900 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.019023]\n",
            "1950 [D loss: 0.000001, acc.: 100.00%] [G loss: 0.009371]\n",
            "2000 [D loss: 0.000002, acc.: 100.00%] [G loss: 0.006087]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}