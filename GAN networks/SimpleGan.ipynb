{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SimpleGan.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6tymJOtvDa6"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Dense, Flatten, Reshape\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZDymesDwHsz"
      },
      "source": [
        "**Model input dimensions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mMiHz6AwN1m"
      },
      "source": [
        "img_rows=28\n",
        "img_cols=28\n",
        "channels=1\n",
        "\n",
        "img_shape=(img_rows,img_cols,channels)\n",
        "\n",
        "#Noise Vector\n",
        "z_dim=100"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Spb52437wptB"
      },
      "source": [
        "**Generator implementation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hk-X3rw6wlvf"
      },
      "source": [
        "def build_generator(img_shape,z_dim):\n",
        "  model=Sequential()\n",
        "  model.add(Dense(128,input_dim=(z_dim)))\n",
        "  model.add(LeakyReLU(alpha=0.01))\n",
        "  model.add(Dense(28*28*1,activation='tanh'))\n",
        "  model.add(Reshape(img_shape))\n",
        "\n",
        "  return model"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzFgzqe0xvPi"
      },
      "source": [
        "**Discriminator implementation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71mwzs-mx2gb"
      },
      "source": [
        "def build_discriminator(img_shape):\n",
        "  model=Sequential()\n",
        "  model.add(Flatten(input_shape=img_shape))\n",
        "  model.add(Dense(128))\n",
        "  model.add(LeakyReLU(alpha=0.01))\n",
        "  model.add(Dense(1,activation='sigmoid'))\n",
        "\n",
        "  return model"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t__eGd4mBnZG"
      },
      "source": [
        "**Building GAN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5z5OhP5BsUs"
      },
      "source": [
        "def build_gan(generator,discriminator):\n",
        "  model=Sequential()\n",
        "  model.add(generator)\n",
        "  model.add(discriminator)\n",
        "\n",
        "  return model\n",
        "\n",
        "discriminator=build_discriminator(img_shape)\n",
        "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(),metrics=['accuracy'])\n",
        "\n",
        "generator=build_generator(img_shape,z_dim)\n",
        "\n",
        "discriminator.trainable=False\n",
        "\n",
        "gan=build_gan(generator,discriminator)\n",
        "\n",
        "gan.compile(loss='binary_crossentropy',optimizer=Adam())\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82SgqB2MDhEf"
      },
      "source": [
        "**GAN Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlfYjP-eDpNd"
      },
      "source": [
        "losses=[]\n",
        "accuracies=[]\n",
        "iteration_checkpoints=[]\n",
        "\n",
        "def train(iterations,batch_size,sample_interval):\n",
        "  (X_train,_) , (_,_)=mnist.load_data()\n",
        "  X_train=X_train/127.5 - 1.0\n",
        "  X_train=np.expand_dims(X_train, axis=3)\n",
        "\n",
        "  real=np.ones((batch_size,1))\n",
        "  fake=np.zeros((batch_size,1))\n",
        "\n",
        "  for iteration in range(iterations):\n",
        "    \n",
        "    idx=np.random.randint(0,X_train.shape[0],batch_size)\n",
        "    imgs=X_train[idx]\n",
        "\n",
        "    z=np.random.normal(0,1,(batch_size,100))\n",
        "    gen_imgs=generator.predict(z)\n",
        "    \n",
        "    d_loss_real=discriminator.train_on_batch(imgs,real)\n",
        "    d_loss_fake=discriminator.train_on_batch(gen_imgs,fake)\n",
        "    d_loss, accuracy=0.5*np.add(d_loss_real,d_loss_fake)\n",
        "\n",
        "    z=np.random.normal(0,1,(batch_size,100))\n",
        "    gen_imgs=generator.predict(z)\n",
        "\n",
        "    g_loss=gan.train_on_batch(z,real)\n",
        "\n",
        "    if (iteration+1) % sample_interval == 0:\n",
        "      losses.append((d_loss,g_loss))\n",
        "      accuracies.append(100.0*accuracy)\n",
        "      iteration_checkpoints.append(iteration+1)\n",
        "\n",
        "      print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (iteration + 1, d_loss, 100.0 * accuracy, g_loss))\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HW-rZyWIJ3RN"
      },
      "source": [
        "**Run the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZGsbPMGeJ8Kf",
        "outputId": "34c7c3a5-2845-4251-84d1-7bddaf842c41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "iterations=10000\n",
        "batch_size=128\n",
        "sample_interval=50\n",
        "\n",
        "train(iterations,batch_size,sample_interval)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50 [D loss: 0.006204, acc.: 100.00%] [G loss: 7.192741]\n",
            "100 [D loss: 0.027969, acc.: 100.00%] [G loss: 3.854657]\n",
            "150 [D loss: 0.037176, acc.: 100.00%] [G loss: 2.763245]\n",
            "200 [D loss: 0.025855, acc.: 100.00%] [G loss: 3.135652]\n",
            "250 [D loss: 0.012532, acc.: 100.00%] [G loss: 3.858117]\n",
            "300 [D loss: 0.006857, acc.: 100.00%] [G loss: 4.566443]\n",
            "350 [D loss: 0.005946, acc.: 100.00%] [G loss: 4.874016]\n",
            "400 [D loss: 0.006660, acc.: 100.00%] [G loss: 5.017665]\n",
            "450 [D loss: 0.005746, acc.: 100.00%] [G loss: 4.876806]\n",
            "500 [D loss: 0.003495, acc.: 100.00%] [G loss: 5.234996]\n",
            "550 [D loss: 0.004231, acc.: 100.00%] [G loss: 5.560380]\n",
            "600 [D loss: 0.004416, acc.: 100.00%] [G loss: 5.812941]\n",
            "650 [D loss: 0.004244, acc.: 100.00%] [G loss: 6.387346]\n",
            "700 [D loss: 0.062967, acc.: 99.22%] [G loss: 3.219258]\n",
            "750 [D loss: 0.018735, acc.: 100.00%] [G loss: 4.754429]\n",
            "800 [D loss: 0.104915, acc.: 98.05%] [G loss: 3.020444]\n",
            "850 [D loss: 0.066195, acc.: 98.44%] [G loss: 4.610833]\n",
            "900 [D loss: 0.086025, acc.: 97.66%] [G loss: 4.278696]\n",
            "950 [D loss: 0.032435, acc.: 99.22%] [G loss: 4.626570]\n",
            "1000 [D loss: 0.039794, acc.: 100.00%] [G loss: 4.667327]\n",
            "1050 [D loss: 0.058881, acc.: 99.61%] [G loss: 4.082499]\n",
            "1100 [D loss: 0.067665, acc.: 98.44%] [G loss: 4.672242]\n",
            "1150 [D loss: 0.040028, acc.: 99.22%] [G loss: 5.358747]\n",
            "1200 [D loss: 0.012030, acc.: 100.00%] [G loss: 5.522941]\n",
            "1250 [D loss: 0.031258, acc.: 98.83%] [G loss: 6.115396]\n",
            "1300 [D loss: 0.016777, acc.: 100.00%] [G loss: 5.205842]\n",
            "1350 [D loss: 0.035276, acc.: 99.22%] [G loss: 5.621929]\n",
            "1400 [D loss: 0.033115, acc.: 99.22%] [G loss: 6.494977]\n",
            "1450 [D loss: 0.050598, acc.: 98.83%] [G loss: 5.513252]\n",
            "1500 [D loss: 0.018142, acc.: 99.61%] [G loss: 7.631262]\n",
            "1550 [D loss: 0.072695, acc.: 98.05%] [G loss: 6.229089]\n",
            "1600 [D loss: 0.070447, acc.: 98.05%] [G loss: 4.754837]\n",
            "1650 [D loss: 0.057801, acc.: 99.61%] [G loss: 5.504970]\n",
            "1700 [D loss: 0.090178, acc.: 96.88%] [G loss: 5.419522]\n",
            "1750 [D loss: 0.192667, acc.: 92.19%] [G loss: 5.177890]\n",
            "1800 [D loss: 0.090743, acc.: 97.27%] [G loss: 6.017997]\n",
            "1850 [D loss: 0.063717, acc.: 97.66%] [G loss: 7.964600]\n",
            "1900 [D loss: 0.142870, acc.: 96.09%] [G loss: 4.148890]\n",
            "1950 [D loss: 0.125024, acc.: 95.31%] [G loss: 5.169211]\n",
            "2000 [D loss: 0.102379, acc.: 96.48%] [G loss: 4.756083]\n",
            "2050 [D loss: 0.134065, acc.: 94.53%] [G loss: 6.974065]\n",
            "2100 [D loss: 0.084073, acc.: 96.48%] [G loss: 4.326727]\n",
            "2150 [D loss: 0.372860, acc.: 82.81%] [G loss: 6.756562]\n",
            "2200 [D loss: 0.085782, acc.: 97.27%] [G loss: 5.863771]\n",
            "2250 [D loss: 0.275246, acc.: 88.67%] [G loss: 4.300604]\n",
            "2300 [D loss: 0.199620, acc.: 91.02%] [G loss: 4.560977]\n",
            "2350 [D loss: 0.284921, acc.: 89.45%] [G loss: 5.407467]\n",
            "2400 [D loss: 0.195061, acc.: 92.58%] [G loss: 4.866893]\n",
            "2450 [D loss: 0.106519, acc.: 96.09%] [G loss: 4.609143]\n",
            "2500 [D loss: 0.228976, acc.: 90.23%] [G loss: 4.347377]\n",
            "2550 [D loss: 0.135296, acc.: 94.14%] [G loss: 5.830532]\n",
            "2600 [D loss: 0.192981, acc.: 90.62%] [G loss: 6.424234]\n",
            "2650 [D loss: 0.121375, acc.: 95.31%] [G loss: 4.914359]\n",
            "2700 [D loss: 0.131819, acc.: 96.09%] [G loss: 3.802063]\n",
            "2750 [D loss: 0.082280, acc.: 97.27%] [G loss: 4.542470]\n",
            "2800 [D loss: 0.167227, acc.: 92.97%] [G loss: 3.929119]\n",
            "2850 [D loss: 0.081258, acc.: 96.88%] [G loss: 5.882980]\n",
            "2900 [D loss: 0.078592, acc.: 97.66%] [G loss: 5.121902]\n",
            "2950 [D loss: 0.230647, acc.: 91.02%] [G loss: 5.079088]\n",
            "3000 [D loss: 0.183807, acc.: 93.75%] [G loss: 4.870797]\n",
            "3050 [D loss: 0.249503, acc.: 91.80%] [G loss: 6.037736]\n",
            "3100 [D loss: 0.168496, acc.: 92.58%] [G loss: 4.251528]\n",
            "3150 [D loss: 0.189263, acc.: 92.58%] [G loss: 5.561860]\n",
            "3200 [D loss: 0.145757, acc.: 94.14%] [G loss: 5.371820]\n",
            "3250 [D loss: 0.269451, acc.: 93.75%] [G loss: 3.571867]\n",
            "3300 [D loss: 0.248813, acc.: 90.62%] [G loss: 4.296649]\n",
            "3350 [D loss: 0.107511, acc.: 96.09%] [G loss: 5.245412]\n",
            "3400 [D loss: 0.128125, acc.: 95.70%] [G loss: 6.054477]\n",
            "3450 [D loss: 0.174207, acc.: 95.70%] [G loss: 6.866915]\n",
            "3500 [D loss: 0.171361, acc.: 92.58%] [G loss: 4.850141]\n",
            "3550 [D loss: 0.136954, acc.: 92.97%] [G loss: 5.635768]\n",
            "3600 [D loss: 0.164219, acc.: 92.58%] [G loss: 4.631451]\n",
            "3650 [D loss: 0.076798, acc.: 98.83%] [G loss: 8.202555]\n",
            "3700 [D loss: 0.069979, acc.: 97.66%] [G loss: 4.847551]\n",
            "3750 [D loss: 0.117264, acc.: 96.88%] [G loss: 6.298875]\n",
            "3800 [D loss: 0.174307, acc.: 92.58%] [G loss: 5.173905]\n",
            "3850 [D loss: 0.200568, acc.: 91.80%] [G loss: 4.292215]\n",
            "3900 [D loss: 0.200500, acc.: 92.58%] [G loss: 5.403853]\n",
            "3950 [D loss: 0.185271, acc.: 93.36%] [G loss: 4.488406]\n",
            "4000 [D loss: 0.130094, acc.: 95.31%] [G loss: 4.167624]\n",
            "4050 [D loss: 0.150197, acc.: 94.92%] [G loss: 5.247908]\n",
            "4100 [D loss: 0.134363, acc.: 94.53%] [G loss: 5.837618]\n",
            "4150 [D loss: 0.151649, acc.: 93.75%] [G loss: 4.658919]\n",
            "4200 [D loss: 0.334065, acc.: 82.81%] [G loss: 4.445856]\n",
            "4250 [D loss: 0.106633, acc.: 96.48%] [G loss: 5.897212]\n",
            "4300 [D loss: 0.227398, acc.: 91.80%] [G loss: 4.806378]\n",
            "4350 [D loss: 0.079980, acc.: 97.27%] [G loss: 5.612666]\n",
            "4400 [D loss: 0.218663, acc.: 91.41%] [G loss: 5.340445]\n",
            "4450 [D loss: 0.117493, acc.: 94.53%] [G loss: 5.770419]\n",
            "4500 [D loss: 0.098783, acc.: 96.48%] [G loss: 4.759971]\n",
            "4550 [D loss: 0.151902, acc.: 95.31%] [G loss: 5.064737]\n",
            "4600 [D loss: 0.118748, acc.: 94.92%] [G loss: 6.765589]\n",
            "4650 [D loss: 0.174924, acc.: 92.58%] [G loss: 4.823718]\n",
            "4700 [D loss: 0.134512, acc.: 94.92%] [G loss: 5.102538]\n",
            "4750 [D loss: 0.167253, acc.: 93.36%] [G loss: 4.778828]\n",
            "4800 [D loss: 0.141981, acc.: 94.14%] [G loss: 7.113699]\n",
            "4850 [D loss: 0.345478, acc.: 87.89%] [G loss: 4.702965]\n",
            "4900 [D loss: 0.136319, acc.: 95.70%] [G loss: 6.250062]\n",
            "4950 [D loss: 0.185654, acc.: 92.19%] [G loss: 5.189859]\n",
            "5000 [D loss: 0.368571, acc.: 85.94%] [G loss: 3.897226]\n",
            "5050 [D loss: 0.274759, acc.: 89.84%] [G loss: 4.646437]\n",
            "5100 [D loss: 0.268872, acc.: 90.23%] [G loss: 5.268350]\n",
            "5150 [D loss: 0.257919, acc.: 91.41%] [G loss: 5.421036]\n",
            "5200 [D loss: 0.135929, acc.: 94.14%] [G loss: 5.834630]\n",
            "5250 [D loss: 0.218364, acc.: 91.02%] [G loss: 5.523117]\n",
            "5300 [D loss: 0.365026, acc.: 83.20%] [G loss: 4.213807]\n",
            "5350 [D loss: 0.186703, acc.: 92.58%] [G loss: 4.679382]\n",
            "5400 [D loss: 0.193178, acc.: 92.58%] [G loss: 3.889120]\n",
            "5450 [D loss: 0.199726, acc.: 92.97%] [G loss: 4.739666]\n",
            "5500 [D loss: 0.124202, acc.: 94.92%] [G loss: 5.285385]\n",
            "5550 [D loss: 0.474306, acc.: 83.98%] [G loss: 4.050040]\n",
            "5600 [D loss: 0.246203, acc.: 91.80%] [G loss: 4.555331]\n",
            "5650 [D loss: 0.229588, acc.: 93.75%] [G loss: 6.211334]\n",
            "5700 [D loss: 0.237084, acc.: 93.36%] [G loss: 5.451042]\n",
            "5750 [D loss: 0.169762, acc.: 94.14%] [G loss: 6.358209]\n",
            "5800 [D loss: 0.183500, acc.: 92.19%] [G loss: 6.185338]\n",
            "5850 [D loss: 0.194389, acc.: 91.41%] [G loss: 4.966245]\n",
            "5900 [D loss: 0.102709, acc.: 96.09%] [G loss: 5.347553]\n",
            "5950 [D loss: 0.311846, acc.: 86.33%] [G loss: 3.854668]\n",
            "6000 [D loss: 0.144529, acc.: 93.75%] [G loss: 5.452198]\n",
            "6050 [D loss: 0.173034, acc.: 93.75%] [G loss: 4.995977]\n",
            "6100 [D loss: 0.435435, acc.: 84.77%] [G loss: 3.796383]\n",
            "6150 [D loss: 0.224986, acc.: 91.02%] [G loss: 3.816918]\n",
            "6200 [D loss: 0.413430, acc.: 83.98%] [G loss: 3.580973]\n",
            "6250 [D loss: 0.238032, acc.: 92.97%] [G loss: 5.478937]\n",
            "6300 [D loss: 0.325962, acc.: 88.67%] [G loss: 3.879185]\n",
            "6350 [D loss: 0.274389, acc.: 90.62%] [G loss: 4.239611]\n",
            "6400 [D loss: 0.481343, acc.: 80.47%] [G loss: 4.047282]\n",
            "6450 [D loss: 0.271802, acc.: 91.02%] [G loss: 3.934776]\n",
            "6500 [D loss: 0.240777, acc.: 89.84%] [G loss: 5.826104]\n",
            "6550 [D loss: 0.273227, acc.: 89.06%] [G loss: 4.461689]\n",
            "6600 [D loss: 0.445995, acc.: 81.25%] [G loss: 3.641576]\n",
            "6650 [D loss: 0.153520, acc.: 93.75%] [G loss: 4.628558]\n",
            "6700 [D loss: 0.251342, acc.: 91.80%] [G loss: 4.941864]\n",
            "6750 [D loss: 0.173804, acc.: 94.14%] [G loss: 4.269215]\n",
            "6800 [D loss: 0.103786, acc.: 97.27%] [G loss: 4.800716]\n",
            "6850 [D loss: 0.274331, acc.: 88.28%] [G loss: 3.991049]\n",
            "6900 [D loss: 0.448492, acc.: 83.59%] [G loss: 2.995641]\n",
            "6950 [D loss: 0.268068, acc.: 89.45%] [G loss: 3.379503]\n",
            "7000 [D loss: 0.238495, acc.: 90.62%] [G loss: 5.111234]\n",
            "7050 [D loss: 0.356141, acc.: 88.67%] [G loss: 3.616184]\n",
            "7100 [D loss: 0.406114, acc.: 82.81%] [G loss: 4.382991]\n",
            "7150 [D loss: 0.254602, acc.: 90.62%] [G loss: 6.698336]\n",
            "7200 [D loss: 0.355373, acc.: 85.55%] [G loss: 3.899858]\n",
            "7250 [D loss: 0.195093, acc.: 91.41%] [G loss: 3.936396]\n",
            "7300 [D loss: 0.306206, acc.: 88.67%] [G loss: 4.343628]\n",
            "7350 [D loss: 0.329646, acc.: 84.77%] [G loss: 4.775877]\n",
            "7400 [D loss: 0.232679, acc.: 87.89%] [G loss: 4.465399]\n",
            "7450 [D loss: 0.279127, acc.: 87.89%] [G loss: 4.043682]\n",
            "7500 [D loss: 0.318135, acc.: 87.11%] [G loss: 4.124711]\n",
            "7550 [D loss: 0.258641, acc.: 89.84%] [G loss: 4.159658]\n",
            "7600 [D loss: 0.245312, acc.: 88.67%] [G loss: 3.681846]\n",
            "7650 [D loss: 0.215681, acc.: 91.80%] [G loss: 3.441424]\n",
            "7700 [D loss: 0.260118, acc.: 90.62%] [G loss: 4.329291]\n",
            "7750 [D loss: 0.461211, acc.: 82.42%] [G loss: 3.767622]\n",
            "7800 [D loss: 0.255647, acc.: 88.28%] [G loss: 5.375098]\n",
            "7850 [D loss: 0.608497, acc.: 77.34%] [G loss: 3.230719]\n",
            "7900 [D loss: 0.316024, acc.: 87.50%] [G loss: 3.363693]\n",
            "7950 [D loss: 0.386696, acc.: 83.59%] [G loss: 3.472360]\n",
            "8000 [D loss: 0.221012, acc.: 90.23%] [G loss: 4.101482]\n",
            "8050 [D loss: 0.318662, acc.: 87.89%] [G loss: 4.658054]\n",
            "8100 [D loss: 0.377546, acc.: 84.77%] [G loss: 3.021893]\n",
            "8150 [D loss: 0.493124, acc.: 80.08%] [G loss: 3.303184]\n",
            "8200 [D loss: 0.229008, acc.: 88.67%] [G loss: 4.421453]\n",
            "8250 [D loss: 0.330428, acc.: 86.72%] [G loss: 4.117301]\n",
            "8300 [D loss: 0.628624, acc.: 75.78%] [G loss: 4.281798]\n",
            "8350 [D loss: 0.491603, acc.: 78.91%] [G loss: 2.938819]\n",
            "8400 [D loss: 0.487412, acc.: 83.98%] [G loss: 4.020115]\n",
            "8450 [D loss: 0.397447, acc.: 83.20%] [G loss: 3.981341]\n",
            "8500 [D loss: 0.315345, acc.: 87.89%] [G loss: 3.749664]\n",
            "8550 [D loss: 0.358160, acc.: 86.72%] [G loss: 3.736636]\n",
            "8600 [D loss: 0.297214, acc.: 87.89%] [G loss: 3.584629]\n",
            "8650 [D loss: 0.363899, acc.: 85.55%] [G loss: 3.400634]\n",
            "8700 [D loss: 0.350047, acc.: 84.77%] [G loss: 2.972552]\n",
            "8750 [D loss: 0.455468, acc.: 80.47%] [G loss: 3.220952]\n",
            "8800 [D loss: 0.191840, acc.: 91.02%] [G loss: 4.583935]\n",
            "8850 [D loss: 0.287869, acc.: 86.72%] [G loss: 4.444437]\n",
            "8900 [D loss: 0.430288, acc.: 82.03%] [G loss: 2.991331]\n",
            "8950 [D loss: 0.336549, acc.: 84.38%] [G loss: 3.797647]\n",
            "9000 [D loss: 0.243472, acc.: 90.62%] [G loss: 4.412696]\n",
            "9050 [D loss: 0.439752, acc.: 82.81%] [G loss: 3.171920]\n",
            "9100 [D loss: 0.419137, acc.: 79.69%] [G loss: 3.595214]\n",
            "9150 [D loss: 0.427437, acc.: 80.86%] [G loss: 3.864481]\n",
            "9200 [D loss: 0.279272, acc.: 87.89%] [G loss: 4.178159]\n",
            "9250 [D loss: 0.255978, acc.: 89.06%] [G loss: 4.230019]\n",
            "9300 [D loss: 0.232847, acc.: 90.23%] [G loss: 5.116899]\n",
            "9350 [D loss: 0.343825, acc.: 86.33%] [G loss: 3.703977]\n",
            "9400 [D loss: 0.625847, acc.: 70.31%] [G loss: 2.523387]\n",
            "9450 [D loss: 0.515116, acc.: 76.56%] [G loss: 3.017505]\n",
            "9500 [D loss: 0.381119, acc.: 84.77%] [G loss: 3.179610]\n",
            "9550 [D loss: 0.258186, acc.: 89.06%] [G loss: 4.190768]\n",
            "9600 [D loss: 0.241257, acc.: 89.45%] [G loss: 4.801270]\n",
            "9650 [D loss: 0.452340, acc.: 82.81%] [G loss: 3.334382]\n",
            "9700 [D loss: 0.421081, acc.: 82.81%] [G loss: 4.172895]\n",
            "9750 [D loss: 0.255809, acc.: 89.45%] [G loss: 4.350271]\n",
            "9800 [D loss: 0.474175, acc.: 78.52%] [G loss: 3.779084]\n",
            "9850 [D loss: 0.247489, acc.: 91.02%] [G loss: 4.315314]\n",
            "9900 [D loss: 0.236846, acc.: 90.62%] [G loss: 3.499292]\n",
            "9950 [D loss: 0.522535, acc.: 78.52%] [G loss: 3.673450]\n",
            "10000 [D loss: 0.131583, acc.: 94.14%] [G loss: 4.212490]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}